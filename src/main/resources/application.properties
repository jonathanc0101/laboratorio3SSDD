server.servlet.context-path=/api

## default connection pool
spring.datasource.hikari.connectionTimeout=20000
spring.datasource.hikari.maximumPoolSize=5
spring.jpa.generate-ddl=false

## PostgreSQL
spring.datasource.url=${SPRING_DATASOURCE_URL}
spring.datasource.username=${SPRING_DATASOURCE_USER}
spring.datasource.password=${SPRING_DATASOURCE_PASSWORD}

#spring.datasource.url=jdbc:postgresql://localhost:5432/borderodb
#spring.datasource.username=borderoadm
#spring.datasource.password=borderoadm


spring.datasource.hikari.schema=public

logging.level.org.springframework.web.filter.CommonsRequestLoggingFilter=DEBUG
logging.level.bordero.client.LoggingInterceptor=DEBUG

bordero.server.id=${BORDERO_SERVER_ID}
#bordero.server.id=A

# Required connection configs for Kafka producer, consumer, and admin
#spring.kafka.properties.sasl.mechanism=PLAIN
spring.kafka.properties.bootstrap.servers=broker:9092
#spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule  required username='D4WJ3IPX5UQ6YOWW' password='T8d508VGhPcQ+J6z8viVydzpzCtTCKjeBFQLvK6G5Q7bBjR2gK/g4iJoQUtK41sx';
#spring.kafka.properties.security.protocol=SASL_SSL

# Best practice for higher availability in Apache Kafka clients prior to 3.0
spring.kafka.properties.session.timeout.ms=45000


spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.IntegerSerializer
spring.kafka.producer.value-serializer=bordero.backend.kafka.serdes.EventSerializer

spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.IntegerDeserializer
spring.kafka.consumer.value-deserializer=bordero.backend.kafka.serdes.EventDeserializer

# Required connection configs for Confluent Cloud Schema Registry
#spring.kafka.properties.basic.auth.credentials.source=USER_INFO
#spring.kafka.properties.basic.auth.user.info={{ SR_API_KEY }}:{{ SR_API_SECRET }}
#spring.kafka.properties.schema.registry.url=https://{{ SR_ENDPOINT }}
